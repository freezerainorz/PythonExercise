{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdBitnFUNP8j"
   },
   "source": [
    "# 透過 Python 擷取網頁上的資料"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1uW2DSxNP8o"
   },
   "source": [
    "在大數據變成顯學的時代，今天我們要做資料分析，若沒有大量的資料，是無法分析出任何有價值的資訊出來的。\n",
    "\n",
    "但是在一般情況下，我們都沒有大量的資料可以做分析，但幸運的是，由於網路的蓬勃發展，獲取任何一個領域的資料比起過往相對就變得容易許多。但是，由於網頁上的資料非常多，若透過手動的方式截取資料，就不是一個很有效率的方法了。\n",
    "\n",
    "因此這時候，透過程式自動從網路上蒐集資料這個問題也變得更加重要，只需一個簡單的程式，就能透過低成本並且自動化的方式從網頁上獲得大量的資料，因此**學習與實作網頁爬蟲成為一個投資報酬率極高的事務**，而 Python 語言由於生態系龐大，套件衆多，也讓用 Python 實作爬蟲比起其他語言相對簡單許多。\n",
    "\n",
    "我們這節課要做的事，就是實作出一個台股股價爬蟲，到[聚財網](https://stock.wearn.com/cdata.asp?Year=110&month=10&kind=2330)上截取資料：\n",
    "\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1m0CQ6zpld2zfwa6T4VJiwIsuVrTTf7V0)\n",
    "\n",
    "最後再將截取到的股價資料輸出至 Excel 工作表：\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=16IhbmwNWHhzaqLTr7KDudDULAay6OLp5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4WAmgQyNP8q"
   },
   "source": [
    "## 請先開啓範例網頁：\n",
    "\n",
    "http://pythonscraping.com/pages/warandpeace.html\n",
    "\n",
    "*以上練習用網頁由 Ryan Mitchell 維護，可以的話，大家買一本他的書支持他繼續維護這個網站：[連結](https://www.books.com.tw/products/0010800965)\n",
    "\n",
    "在進入網頁之後，試試看直接點擊網頁，然後右鍵 -> 儲存，接下來瀏覽器會將一個副檔名為 **html** 的檔案存下來。由此我們發現，任何一個我們看得見的網頁，其實都是一個副檔名為 **.html** 的檔案，問題是，這個 **html** 檔案是從何而來？它的原理又是什麽？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irY_iD9GNP8r"
   },
   "source": [
    "## 網絡基礎： HTTP 溝通協定\n",
    "\n",
    "HTTP 用白話講，就像是**電腦與電腦之間的共同語言**，電腦需要通過這個共同語言，才能在網絡上面互相溝通。\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1ROkz8I9UTnnRBI_8SkcPb1IXZd7leG-q)\n",
    "\n",
    "而一個 HTTP 的請求 (Request) 是根據 **HTTP 動詞**(HTTP Verb) 以及**網址**才能運作，舉例來説，當你在輸入 https://www.google.com/ ，連至 Google 的首頁時，整個背後的運作流程是：\n",
    "\n",
    "1. 用戶端 (你的瀏覽器) 針對 Google 的雲端伺服器發送了一個 GET Request\n",
    "2. 而 Google 的雲端伺服器在收到請求後，將需要呈現該網頁的資料都計算完成\n",
    "3. 接著 Google 的雲端伺服器會回傳一個回應 (Response)，這個 Response 内通常就包含了一個 html 檔案\n",
    "4. 用戶端 (你的瀏覽器) 在下載了請求回傳的 html 檔案之後，將 html 程式碼渲染成網頁，呈現給使用者\n",
    "\n",
    "## HTTP Request 的種類\n",
    "\n",
    "一般來説，Request 有 GET 與 POST：\n",
    "\n",
    "- GET 代表我需要查詢 / 顯示資料，像是 GET https://www.facebook.com/ 代表查詢 facebook 首頁\n",
    "- POST 代表我需要新增資料，通常用於網頁上的表單 POST https://www.foodpanda.com/orders 代表新增訂單\n",
    "\n",
    "## HTTP response 的種類\n",
    "\n",
    "- 一般伺服器通常是回傳 html 網頁檔案\n",
    "- 但若是一些功能像是下載 / 輸出報表，Response 則是一個 xlsx / csv 檔案\n",
    "\n",
    "\n",
    "## 從 Excel 的角度來理解...\n",
    "\n",
    "像是 FB, Google, Yahoo 等網站，**其實都像是一個個運行在雲端上的 Excel 函數**。\n",
    "\n",
    "而要使用該函數時，就必須透過一個 HTTP 的請求 (Request) 來呼叫該函數。讓瀏覽器發送 HTTP Request 的方法就是輸入網址，就像是在 Excel 上輸入公式一樣。\n",
    "\n",
    "而該公式若執行成功，不同於 Excel 是將結果顯示在工作表上，HTTP Request 的結果就是一個 Response，該 Response 通常是顯示在瀏覽器上的網頁，也有可能是 xlsx / csv 檔案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXd_i_QONP8t"
   },
   "source": [
    "\n",
    "## 用 Python 實作爬蟲\n",
    "\n",
    "Python 用來實作爬蟲的兩個主流套件：\n",
    "\n",
    "- BeautifulSoup [官方文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
    "\n",
    "- PyQuery [官方文件](https://pythonhosted.org/pyquery/)\n",
    "\n",
    "簡單來説：\n",
    "\n",
    "**BeautifulSoup** 是比較貼近**程式設計師**的角度去思考\n",
    "\n",
    "**PyQuery** 貼近**網頁開發者**的角度去思考\n",
    "\n",
    "在這堂課考量到並不是每一個人都具備網頁開發的背景，因此我們會使用 BeautifulSoup 套件來實作爬蟲\n",
    "\n",
    "## 範例網頁：\n",
    "\n",
    "網址：http://pythonscraping.com/pages/warandpeace.html\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1UdPdrudjm8GUcWMsEehLcl_TjxVv7DF1)\n",
    "\n",
    "*以上練習用網頁由 Ryan Mitchell 維護，可以的話，大家可以買一本書支持他：[連結](https://www.books.com.tw/products/0010800965)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac0N9fsvNP8u"
   },
   "source": [
    "## 網頁開發 101\n",
    "\n",
    "任何網頁都是由 **html 標籤(tag)** 所組成，基本結構如下\n",
    "```html\n",
    "<標籤名稱 class=\"類別名稱\">內容</標籤名稱>\n",
    "<標籤名稱 id=\"id名稱\">內容</標籤名稱>\n",
    "```\n",
    "今天我們要擷取的任何內容，一定是被包裹在在某一個標籤裡面\n",
    "而今天若網頁開發者需要改變任何一個標籤的**樣式**，就需要用到 **css** 語法\n",
    "以上面的網頁為例，人名都是以綠色顯示，所以就先宣告一個名為 **green** 的 css 類別:\n",
    "\n",
    "```html\n",
    "<style>\n",
    ".green{\n",
    "\tcolor:#55ff55;\n",
    "}\n",
    "</style>\n",
    "```\n",
    "\n",
    "若今天希望讓一個標籤的内容文字變成綠色，可以使用定義好的 .green 這個 css 類別：\n",
    "\n",
    "```html\n",
    "<span class=\"green\">Prince Vasili Kuragin</span>\n",
    "```\n",
    "\n",
    "*想了解更多 html 可以看一下 Mozilla 官網的教學：[HTML 基礎](https://developer.mozilla.org/zh-TW/docs/Learn/Getting_started_with_the_web/HTML_basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 來實作吧...\n",
    "\n",
    "今天我們要截取網頁内一些特定的内容，我們必須先將網頁下載下來做分析：\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# 針對網頁發送 GET Reqeust\n",
    "res = requests.get(\"http://pythonscraping.com/pages/warandpeace.html\")\n",
    "# 將回傳的 Response 内的文字顯示出來\n",
    "res.text\n",
    "#'<html>\\n<head>\\n<style>\\n.green{\\n\\tcolor:#55ff55;\\n}\\n.red{\\n\\tcolor:#ff5555;\\n}\\n#text{\\n\\twidth:50%;\\n}\\n</style>\\n</head>\\n<body>\\n<h1>War and Peace</h1>\\n<h2>Chapter 1</h2>\\n<div id=\"text\">\\n\"<span class=\"red\">Well, Prince, so Genoa and Lucca are now just family estates of the\\nBuonapartes. But I warn you, if you don\\'t...\n",
    "```\n",
    "\n",
    "可以看到我們的範例網頁的 html 程式碼是被濃縮成了一個龐大的字串，而我們想要截取的資料一定是在這個字串内的某個地方，接下來就要思考如何能夠在這個字串過濾出我們有興趣的資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 -> http status code\n",
    "#404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ov5EcSJNP8w"
   },
   "source": [
    "## 使用 標籤 「類別名稱」取得資料\n",
    "\n",
    "要擷取資料前，首先需要透過方法**選擇**到該標籤\n",
    "##  BeautifulSoup 套件\n",
    "\n",
    "典故來自 Alice in WonderLand 裏面一首同名的詩，由假海龜 (Mock Turtle) 所唱，影射英國料理假海龜湯...\n",
    "\n",
    "有興趣自己可以去 Google, 不多説了...\n",
    "\n",
    "想查看 BeautifulSoup 套件的功能請看一下官方的中文文件：\n",
    "\n",
    "[BeautifulSoup 官方文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByfQTMtkNP8x"
   },
   "source": [
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 針對網頁發送 GET Reqeust\n",
    "res = requests.get(\"http://pythonscraping.com/pages/warandpeace.html\")\n",
    "# 將回傳的 Response 内的文字用 BeautifulSoup 解析\n",
    "html = BeautifulSoup(res.text, \"html.parser\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下來我們希望將放置人物名稱的標籤，也就是 **span** 標籤，搜尋出來：\n",
    "\n",
    "```python\n",
    "name_list = html.findAll(\"span\")\n",
    "print(name_list)\n",
    "```\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1orRZFAFrzGoHL5M-Wnkiwakhg2hyj1TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho8oDCIuNP8y",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是我們馬上發現到，範例網頁内的除了人名外，還有人物的對白也是被放在 `span` 標籤内，我們截取到了很多不需要的東西，所以希望能夠增加一些過濾的條件，我們注意到了所有人物標籤的 `class` 屬性皆爲 **green**，所以我們就修改我們原本的程式碼：\n",
    "\n",
    "```python\n",
    "name_list = html.findAll(\"span\", { \"class\": \"green\"})\n",
    "print(name_list)\n",
    "#[<span class=\"green\">Anna\n",
    "# Pavlovna Scherer</span>, <span class=\"green\">Empress Marya\n",
    "# Fedorovna</span>, <span class=\"green\">Prince Vasili Kuragin</span>, <span class=\"green\">Anna # Pavlovna</span>, <span class=\"green\">St. Petersburg</span>,\n",
    "```\n",
    "\n",
    "讓 BeautifulSoup 過濾出所有 `class` 為 **green** 的 `span` 標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXHpzZlHNP85",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 截取出標籤内的文字\n",
    "\n",
    "現在我們截取到的，是一個 BeautifulSoup 的 html 標籤物件的集合\n",
    "\n",
    "一個 html 標籤物件其實是個複雜的存在，如同包含很多不同的屬性\n",
    "\n",
    "現在我們希望能夠將每一個 html 標籤中的文字提取出來：\n",
    "\n",
    "```python\n",
    "# 提取出第一個 html 標籤物件\n",
    "tag = name_list[0]\n",
    "# 截取該標籤内的文字\n",
    "tag.text\n",
    "```\n",
    "\n",
    "確認以上方法可以提取到資料之後，我們就來寫個 for loop 將每一個 html 標籤的文字印出來：\n",
    "\n",
    "```python\n",
    "for tag in name_list:\n",
    "    print(tag.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將截取到的資料寫入 Excel\n",
    "\n",
    "最後我們希望將截取到的資料以下圖的格式呈現在 Excel 内：\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1D4GhIoQj1Mx5k-atpgpFKdhzu4gTKdkg)\n",
    "\n",
    "```python\n",
    "import xlwings as xw\n",
    "# 開啓新的工作簿以及工作表\n",
    "wb = xw.Book()\n",
    "sheet = wb.sheets[0]\n",
    "# 將 A 欄表頭設定為 “出場人物” \n",
    "sheet.range(\"A1\").value = \"出場人物\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過迴圈，將資料寫入 A 欄每一個儲存格：\n",
    "\n",
    "```python\n",
    "# 從第二列開始\n",
    "row = 2\n",
    "# 將每一個 span 標籤内的文字寫入 A 欄\n",
    "for name in name_list:\n",
    "    sheet.range(f\"A{row}\").value = name.text\n",
    "    row += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自動偵測欄寬sheet.range(\"A:A\").autofit()\n",
    "#存檔 wb.save(\"出場人物.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyArZIqTNP8-"
   },
   "source": [
    "# 網頁爬蟲實戰：(臺股爬蟲)\n",
    "\n",
    "我們想要截取資料的網頁：[聚財網](https://stock.wearn.com/cdata.asp?Year=110&month=10&kind=2330)\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1cubJznOW90bzY4GaALxhDXLNO396oa-_)\n",
    "\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://stock.wearn.com/cdata.asp?kind=2330\")\n",
    "# 設定目標網頁的編碼\n",
    "res.encoding = \"big5\"\n",
    "# 用 html 格式解碼 爬下來的檔案\n",
    "html = BeautifulSoup(res.text, \"html.parser\")\n",
    "print(html)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxK2DxaPNP8_",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析一下我們要爬的網頁\n",
    "\n",
    "收盤價是被封裝在一個 **table** 標籤内部的一個 **td** 標籤\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1u7G9DPd0m7YihucnpJTPaGvkgjk-3DWr)\n",
    "\n",
    "\n",
    "## table 標籤\n",
    "\n",
    "一般網頁若要呈現重要的資訊，都會將資訊以網頁表格的形式呈現\n",
    "\n",
    "今天若要利用 **html** 實作網頁上的表格，需要名爲 **table** 的標籤\n",
    "\n",
    "換句話説，今天網頁上我們有興趣的資料，十之八九都是被封裝在 **table** 底下\n",
    "\n",
    "把 **table** 的結構搞懂，就成了一件重要的事情。\n",
    "\n",
    "## html table 標籤的結構\n",
    "\n",
    "網頁上的資料大多都是匯整在表格、而 html 的表格則是由 table 標籤構成的：\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=16_rSO2jx4HWkXSKnklsSOxKTwpsUUcwy)\n",
    "\n",
    "該 **table** 標籤内有兩個 **tr** 標籤，每一個 **tr** 標籤是對應到表格上的一列（row）。\n",
    "\n",
    "第二個 **tr** 標籤内的第一個 **td** 標籤是我們要的收盤價\n",
    "\n",
    "\n",
    "## html 標簽的關聯\n",
    "\n",
    "簡單來説，就是一個樹狀圖的概念：\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1n2yZlQtj7EK2_NNIBXvEVGeEKBogfgYZ)\n",
    "\n",
    "---\n",
    "## 延申閲讀\n",
    "\n",
    "HTML Table 教學\n",
    "\n",
    "w3 school： [連結](https://www.w3schools.com/html/html_tables.asp)\n",
    "\n",
    "Mozilla：[連結](https://developer.mozilla.org/zh-TW/docs/Web/HTML/Element/table)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFh0F2spNP9D"
   },
   "source": [
    "## 開始實作台股爬蟲\n",
    "\n",
    "```python\n",
    "html.findAll(\"table\")\n",
    "# 我們就發現網頁内有多個 table...\n",
    "len(html.findAll(\"table\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBGjdatZNP9E"
   },
   "outputs": [],
   "source": [
    "#table = html.findall(\"table\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59hS4xnVaDCE"
   },
   "source": [
    "## 檢查所有搜尋到的 table\n",
    "\n",
    "仔細觀察一下，我們發現目標 table 的 **width（寬度）** 屬性是被設定成 **520**\n",
    "\n",
    "因此我們可以將程式碼修改爲...\n",
    "\n",
    "```python\n",
    "tables = html.findAll(\"table\", { \"width\": \"520\"})\n",
    "print(tables)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取出串列内唯一的 table\n",
    "\n",
    "\n",
    "```python\n",
    "tables = html.findAll(\"table\", { \"width\": \"520\"})\n",
    "table = tables[0]\n",
    "print(table)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以先將每一列内，也就是每一個 **tr** 標籤内的 **td** 標籤讀取出來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意我們要的資料在第三列，也就是 **table** 標籤内的第三個 **tr** 標籤内：\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1K_Ph8-oWN6SXQS4V1Kx6Tn3fXCrJfmbv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查單一 tr 標籤内的 td 標籤\n",
    "\n",
    "```python\n",
    "tr = trs[2]\n",
    "data_row = tr.findAll(\"td\")\n",
    "print(data_row)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡化一下我們的程式碼\n",
    "\n",
    "使用串列生成式：\n",
    "\n",
    "```python\n",
    "tds = [tr.findAll(\"td\") for tr in trs]\n",
    "print(tds)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們要是檢查一下每一個 **td** 的内容：\n",
    "\n",
    "```python\n",
    "data_row[1].text\n",
    "# '598.00\\xa0\\xa0\\xa0'\n",
    "```\n",
    "\n",
    "就會發現内容的字串結尾是三個 **\\xa0** 字元，也就是網頁的**不換行空格（\\&nbsp）**，同學可以參考 [這篇文章](https://note.charlestw.com/html-space/) 對網頁空格的介紹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以透過 **replace** 函數去除，\n",
    "\n",
    "```python\n",
    "data_row[1].text.replace(\"\\xa0\", \"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們的開盤價就是：\n",
    "\n",
    "```python\n",
    "price_open = float(data_row[1].text)\n",
    "print(price_open)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而成交量則是需要將字串内的 **\",\"** 符號去除掉：\n",
    "\n",
    "```python\n",
    "volume = float(data_row[5].text.replace(\",\", \"\"))\n",
    "print(volume)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = html.findAll(\"table\")[0]\n",
    "tr = table.findAll(\"tr\")[2] #第三個數(0,1,2)\n",
    "tds = tr.findAll(\"td\")\n",
    "price_open = float(tds[1].text.replace(\"\\xa0\", \"\"))#float = 浮點數\n",
    "print(price_open)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隨堂練習\n",
    "\n",
    "請試試看將 2330 的開盤價、最高價、最低價、成交價、以及成交量截取出來：\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://stock.wearn.com/a2330.html\")\n",
    "res.encoding = \"big5\"\n",
    "html = BeautifulSoup(res.text, \"html.parser\")\n",
    "table = html.findAll(\"table\", { \"width\": \"380\" })[0]\n",
    "tds = table.findAll(\"tr\")[1].findAll(\"td\")\n",
    "\n",
    "price_open = ________________\n",
    "price_high = ________________\n",
    "price_low = ________________\n",
    "price_close = ________________\n",
    "volume = ________________\n",
    "\n",
    "print(f\"台積電收盤價：${price_close}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlFWh49oNP9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡化一下我們的程式碼\n",
    "\n",
    "```python\n",
    "data = [float(td.text.replace(\"\\xa0\", \"\").replace(\",\", \"\")) for td in data_row[1:]]\n",
    "print(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvX3ORI1NP9i"
   },
   "source": [
    "## 將結果寫入 Excel\n",
    "\n",
    "用 xlwings 開啓 **TW2330.xlsx**：\n",
    "\n",
    "```python\n",
    "import xlwings as xw\n",
    "import time\n",
    "\n",
    "wb = xw.Book(r\"TW2330.xlsx\")\n",
    "sheet = wb.sheets[\"data\"]\n",
    "```\n",
    "\n",
    "偵測最後一個 row:\n",
    "\n",
    "```python\n",
    "last_row = sheet.range(\"A1\").end(\"down\").row\n",
    "last_row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcThVQ8XNP9j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcvNEFsnNP9l"
   },
   "source": [
    "## 產生格式化的時間字串\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "time.strftime(\"%Y/%m/%d\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb1Lg6kVNP9n"
   },
   "outputs": [],
   "source": [
    "#string format time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9FWOzKDaDC8"
   },
   "source": [
    "## 將時間與開盤價寫入 Excel \n",
    "\n",
    "```python\n",
    "sheet.range(f\"A{last_row+1}\").value = time.strftime(\"%Y/%m/%d\")\n",
    "sheet.range(f\"B{last_row+1}\").value = closing_price\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b8G_zA0aDC-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGZboTzgNP9r"
   },
   "source": [
    "## 完成版程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQvsI2xYNP9v"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import xlwings as xw\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://stock.wearn.com/cdata.asp?kind=2330\")\n",
    "# 設定目標網頁的編碼\n",
    "res.encoding = \"big5\"\n",
    "# 用 html 格式解碼 爬下來的檔案\n",
    "html = BeautifulSoup(res.text, \"html.parser\")\n",
    "trs = table.findAll(\"tr\")\n",
    "tds = [tr.findAll(\"td\") for tr in trs]\n",
    "today = tds[2]\n",
    "data = [float(td.text.replace(\"\\xa0\", \"\").replace(\",\", \"\")) for td in today[1:]]\n",
    "\n",
    "price_open = data[0]\n",
    "price_high = data[1]\n",
    "price_low = data[2]\n",
    "price_close = data[3]\n",
    "volume = data[4]\n",
    "\n",
    "# 將台積電資料寫入 Excel\n",
    "# 開啓 Excel 檔\n",
    "wb = xw.Book(r\"tw_stock_portfolio.xlsx\")\n",
    "sheet = wb.sheets[\"2330\"]\n",
    "# 偵測工作表最後一行\n",
    "last_row = sheet.range(\"A1\").end(\"down\").row\n",
    "# 將截取到的資料寫入工作表\n",
    "sheet.range(f\"A{last_row+1}\").value = time.strftime(\"%Y/%m/%d\")\n",
    "sheet.range(f\"B{last_row+1}\").value = price_open\n",
    "sheet.range(f\"C{last_row+1}\").value = price_high\n",
    "sheet.range(f\"D{last_row+1}\").value = price_low\n",
    "sheet.range(f\"E{last_row+1}\").value = price_close\n",
    "sheet.range(f\"F{last_row+1}\").value = volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 截取過去一個月的歷史股價\n",
    "\n",
    "由於資料在原始網頁上的排序方式是倒序（越早的價格排越下面），與我們的 Excel 格式相反，因此我們可以先改變 **tr** 的順序：\n",
    "\n",
    "```python\n",
    "data_rows = [tr.findAll(\"td\") for tr in trs][2:]\n",
    "data_rows.reverse()\n",
    "data_rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認順序沒有問題之後，我們就可以透過一個迴圈處理每一個 **tr** 内的資料：\n",
    "\n",
    "\n",
    "```python\n",
    "data_rows = [tr.findAll(\"td\") for tr in trs][2:]\n",
    "data_rows.reverse()\n",
    "\n",
    "for tds in data_rows:\n",
    "    data = [td.text.replace(\"\\xa0\", \"\") for td in tds]\n",
    "    y, m, d = data[0].split('/')\n",
    "    date = str(int(y)+1911) + '/' + m  + '/' + d\n",
    "    price_open = float(data[1])\n",
    "    price_high = float(data[2])\n",
    "    price_low = float(data[3])\n",
    "    price_close = float(data[4])\n",
    "    volume = int(data[5].replace(\",\", \"\"))\n",
    "    print((date, price_open, price_high, price_low, price_close, volume))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成版程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import xlwings as xw\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://stock.wearn.com/cdata.asp?kind=2330\")\n",
    "# 設定目標網頁的編碼\n",
    "res.encoding = \"big5\"\n",
    "# 用 html 格式解碼爬下來的檔案\n",
    "html = BeautifulSoup(res.text, \"html.parser\")\n",
    "trs = table.findAll(\"tr\")\n",
    "data_rows = [tr.findAll(\"td\") for tr in trs][2:]\n",
    "data_rows.reverse()\n",
    "\n",
    "# 將台積電資料寫入 Excel\n",
    "# 開啓 Excel 檔\n",
    "wb = xw.Book(r\"tw_stock_portfolio.xlsx\")\n",
    "sheet = wb.sheets[\"2330\"]\n",
    "# 偵測工作表最後一行\n",
    "last_row = sheet.range(\"A1\").end(\"down\").row\n",
    "\n",
    "for tds in data_rows:\n",
    "    data = [td.text.replace(\"\\xa0\", \"\") for td in tds]\n",
    "    y, m, d = data[0].split('/')\n",
    "    # 將截取到的資料寫入工作表\n",
    "    sheet.range(f\"A{last_row+1}\").value = str(int(y)+1911) + '/' + m  + '/' + d\n",
    "    sheet.range(f\"B{last_row+1}\").value = float(data[1])\n",
    "    sheet.range(f\"C{last_row+1}\").value = float(data[2])\n",
    "    sheet.range(f\"D{last_row+1}\").value = float(data[3])\n",
    "    sheet.range(f\"E{last_row+1}\").value = float(data[4])\n",
    "    sheet.range(f\"F{last_row+1}\").value = int(data[5].replace(\",\", \"\"))\n",
    "    # 遞增 last_row\n",
    "    last_row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今天若想要把整個投資組合的資料截取到 Excel 内\n",
    "\n",
    "我們發現到，在 Yahoo 奇摩股市的網頁上，不同股票的資料**都會顯示在 html 結構一樣的網頁上**，而唯一不同的只有**網址**，我們可以透過一個迴圈，將一整個投資組合的資料截取下來，並存入相對應的工作表内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import xlwings as xw\n",
    "\n",
    "wb = xw.Book(r\"tw_stock_portfolio.xlsx\")\n",
    "date = time.strftime(\"%Y/%m/%d\")\n",
    "# 定義所有投資組合的股票代號\n",
    "stocks = [\"2330\", \"0050\", \"2412\", \"2317\", \"2603\"]\n",
    "\n",
    "for stock in stocks:\n",
    "    # 截取到該股票代號的收盤價\n",
    "    res = requests.get(f\"https://stock.wearn.com/cdata.asp?kind={stock}\")\n",
    "    # 設定目標網頁的編碼\n",
    "    res.encoding = \"big5\"\n",
    "    # 用 html 格式解碼爬下來的檔案\n",
    "    html = BeautifulSoup(res.text, \"html.parser\")\n",
    "    trs = table.findAll(\"tr\")\n",
    "    data_rows = [tr.findAll(\"td\") for tr in trs][2:]\n",
    "    data_rows.reverse()\n",
    "    # 截取相對應的工作表\n",
    "    sheet = wb.sheets[stock]\n",
    "    # 偵測該工作表的最後一行行數\n",
    "    last_row = sheet.range(\"B1\").end(\"down\").row\n",
    "\n",
    "    for tds in data_rows:\n",
    "        data = [td.text.replace(\"\\xa0\", \"\") for td in tds]\n",
    "        y, m, d = data[0].split('/')\n",
    "        # 將截取到的資料寫入工作表\n",
    "        sheet.range(f\"A{last_row+1}\").value = str(int(y)+1911) + '/' + m  + '/' + d\n",
    "        sheet.range(f\"B{last_row+1}\").value = float(data[1])\n",
    "        sheet.range(f\"C{last_row+1}\").value = float(data[2])\n",
    "        sheet.range(f\"D{last_row+1}\").value = float(data[3])\n",
    "        sheet.range(f\"E{last_row+1}\").value = float(data[4])\n",
    "        sheet.range(f\"F{last_row+1}\").value = int(data[5].replace(\",\", \"\"))\n",
    "        # 遞增 last_row\n",
    "        last_row += 1\n",
    "\n",
    "# 存檔\n",
    "wb.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tUUe7yTNP9y"
   },
   "source": [
    "# 小結：\n",
    "\n",
    "1. 學習與實作網頁爬蟲是一個**投資報酬率極高的事務**\n",
    "2. Python 語言由於使用者衆多，**與爬蟲相關的套件、解決方案、與教學也多，讓實作變得相對簡單**\n",
    "3. 實作上，最困難的部分在於**解析網頁的 html 結構**\n",
    "4. 網頁的資料很大的機率都是被封裝在 **table** 這個 html 標簽下\n",
    "5. 但若今天**網頁改版，原先寫好的爬蟲就有可能截取不到資料**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwXBkvxCNP9z"
   },
   "source": [
    "# 功課：匯率爬蟲\n",
    "\n",
    "請寫一個網頁爬蟲，截取臺灣銀行牌告匯率網頁：\n",
    "\n",
    "http://rate.bot.com.tw/xrt?Lang=zh-TW\n",
    "\n",
    "\n",
    "並且將匯率資料用以下格式呈現在 Excel 内：\n",
    "![](https://drive.google.com/uc?export=download&id=1YCl-QcAJCW951AhosB3HhuV7Fwy3hjMZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUCCt1ObNP90"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Python 程式設計 第七課：網頁爬蟲.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
